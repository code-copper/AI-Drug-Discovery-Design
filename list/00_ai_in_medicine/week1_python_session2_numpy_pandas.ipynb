{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# 人工智能在医学中的应用：数据科学--基础2\n",
    "\n",
    "## Python 编程：\"numpy\" 和 \"pandas\"\n",
    "\n",
    "- 讲师：itwangyang (itwangyang@gmail.com)\n",
    "\n",
    "- 目标受众：医学生\n",
    "\n",
    "- 课程日期：2024 年 09 月 13 日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.学习目标\n",
    "\n",
    "在本次学习中，您将接触到**数据科学。您将使用**Python软件包 \"numpy \"和 \"pandas\"，加载并处理COVID-19数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.学习目标\n",
    "#### 理论\n",
    "* 数据科学\n",
    "* `numpy` 库\n",
    "* `pandas`库\n",
    "### 实用\n",
    "- 1.数据集\n",
    "- 2.用 `pandas` 作为 `DataFrame` 读取数据\n",
    "- 3.查看数据\n",
    "- 4.选择列\n",
    "- 5.获取列中的唯一条目\n",
    "- 6.选择行\n",
    "- 7.分组数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.参考文献\n",
    "\n",
    "- 数据科学、机器学习、人工智能\n",
    "    - http://varianceexplained.org/r/ds-ml-ai/\n",
    "- 向量、矩阵、张量\n",
    "    - https://www.quantstart.com/articles/scalars-vectors-matrices-and-tensors-linear-algebra-for-deep-learning-part-1/\n",
    "    - https://dev.to/mmithrakumar/scalars-vectors-matrices-and-tensors-with-tensorflow-2-0-1f66\n",
    "- `numpy`\n",
    "    - https://numpy.org/doc/stable/user/absolute_beginners.html\n",
    "    - https://scipy-lectures.org/intro/numpy/array_object.html\n",
    "- `pandas`\n",
    "    - https://medium.com/dunder-data/how-to-learn-pandas-108905ab4955\n",
    "    - https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/#iloc-selection\n",
    "    - https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-6fcd0170be9c\n",
    "- 数据集\n",
    "    - COVID-19 病例：[原始 RKI 数据](https://opendata.arcgis.com/datasets/dd4580c810204019a7b8eb3e0b329dd6_0.csv)\n",
    "    - 德国疫苗接种进展情况：[原始 RKI 数据](https://www.rki.de/DE/Content/InfAZ/N/Neuartiges_Coronavirus/Daten/Impfquoten-Tab.html) 和[处理后数据](https://github.com/ard-data/2020-rki-impf-archive)\n",
    "- 数据可视化\n",
    "    - RKI COVID-19 面板][https://corona.rki.de/]\n",
    "    - [COVID-19 柏林各区案例 > Bezirke > Übersicht](https://www.berlin.de/corona/lagebericht/)\n",
    "    - [疫苗接种仪表板](https://impfdashboard.de/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.理论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据科学"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据科学、机器学习和人工智能之间有什么区别？\n",
    "\n",
    "改编自 [David Robinson 的博文](http://varianceexplained.org/r/ds-ml-ai/)。\n",
    "\n",
    "数据科学、机器学习和人工智能领域确实有很多***重叠之处，但它们***不能互换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **数据科学**产生**见解**\n",
    "- \"普通患者的存活几率为 70%\"（描述性：描述数据集）\n",
    "- \"不同的病人有不同的存活几率\"（探索性：发现你不知道的关系）\n",
    "- 随机实验表明，分配给 Alice 的病人比分配给 Bob 的病人更有可能存活\"（相关性：找出一个变量在另一个变量发生变化时的情况）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **机器学习**（ML）产生**预测**\n",
    "\n",
    "- \"预测这名患者是否会患上败血症\"\n",
    "- \"预测这张图片中是否有一只鸟\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **人工智能**（AI）产生**行动**\n",
    "\n",
    "- 游戏算法（深蓝、AlphaGo）\n",
    "- 机器人学和控制理论（运动规划、双足机器人行走）\n",
    "- 优化（谷歌地图选择路线）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `numpy` 库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 概览\n",
    "\n",
    "* 角色：科学计算（使用阵列）\n",
    "* 网站： https://numpy.org/\n",
    "* 说明（摘自 [此处](https://numpy.org/doc/stable/user/absolute_beginners.html)）：\n",
    "> NumPy (Numerical Python) is an open source Python library that’s used in almost every field of science and engineering. It’s the universal standard for working with numerical data in Python, and it’s at the core of the scientific Python and PyData ecosystems. NumPy users include everyone from beginning coders to experienced researchers doing state-of-the-art scientific and industrial research and development. The NumPy API is used extensively in Pandas, SciPy, Matplotlib, scikit-learn, scikit-image and most other data science and scientific Python packages.\n",
    "* 文献资料：https://numpy.org/devdocs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 应用\n",
    "\n",
    "- 以数组形式创建向量（一维）、矩阵（二维）和张量（>=三维）\n",
    "- 使用大量高级数学函数对这些数组进行操作\n",
    "- 在 \"pandas\"、\"scipy\"、\"matplotlib\"、\"scikit-learn \"和大多数其他数据科学和科学 Python 软件包中广泛使用\n",
    "![](https://res.cloudinary.com/practicaldev/image/fetch/s--oTgfo1EL--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://raw.githubusercontent.com/adhiraiyan/DeepLearningWithTF2.0/master/notebooks/figures/fig0201a.png)\n",
    "\n",
    "图源：https://dev.to/mmithrakumar/scalars-vectors-matrices-and-tensors-with-tensorflow-2-0-1f66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pandas` 库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 概览\n",
    "\n",
    "* 角色：数据处理和分析\n",
    "* 网站： https://pandas.pydata.org/\n",
    "* 说明（摘自 [此处](https://pandas.pydata.org/)）：\n",
    "> pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
    "built on top of the Python programming language.\n",
    "* 文档：https://pandas.pydata.org/pandas-docs/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 应用\n",
    "\n",
    "摘自：https://medium.com/dunder-data/how-to-learn-pandas-108905ab4955\n",
    "\n",
    "> `pandas` 可以完成许多任务，包括\n",
    "\n",
    ">\n",
    "\n",
    "> 读取/写入多种不同的数据格式\n",
    "\n",
    "> 选择数据子集\n",
    "\n",
    "> 跨行和跨列计算\n",
    "\n",
    "> 查找和填补缺失数据\n",
    "\n",
    "> 对数据中的独立组进行操作\n",
    "\n",
    "> 将数据重塑为不同的形式\n",
    "\n",
    "> 通过 matplotlib 和 seaborn 实现可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `DataFrame`和`Series`。\n",
    "\n",
    "pandas \"库有两个主要的数据容器：\"DataFrame\"（二维）和 \"Series\"（一维）。\n",
    "\n",
    "- `DataFrame` [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html):\n",
    "  > Two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns). Arithmetic operations align on both row and column labels. Can be thought of as a dict-like container for Series objects. The primary pandas data structure\n",
    "- `Series` [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html):\n",
    "  > One-dimensional ndarray with axis labels (including time series).\n",
    "\n",
    "\n",
    "数据帧 \"比 \"系列 \"使用得更多，让我们来看看它的组成部分。\n",
    "\n",
    "![DataFrame anatomy](https://raw.githubusercontent.com/volkamerlab/ai_in_medicine/master/images/dataframe_anatomy.png)\n",
    "\n",
    "图源：https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-6fcd0170be9c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.实用性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>我们的目标：</b>我们将介绍熊猫的所有功能，这些功能是我们按年龄组和地区可视化柏林 COVID-19 最新病例数所必需的。在了解了如何实现可视化之后，您将获得德国疫苗接种进展的最新数据，并亲自绘制首次/第二次疫苗接种的时间进程图。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.数据集\n",
    "我们将使用罗伯特-科赫研究所（RKI）每天发布的 COVID-19 病例数据，这些数据在 RKI COVID-19 Dashboard(https://corona.rki.de) 上可以非常直观地显示出来。在本笔记本中，我们将重点关注柏林的数据。\n",
    "\n",
    "该数据集可在此处免费获取：我们可以通过以下网址将数据集直接加载到pandas中： https://opendata.arcgis.com/datasets/dd4580c810204019a7b8eb3e0b329dd6_0.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.用 `pandas` 作为 `DataFrame` 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们导入库 `numpy` 和 `pandas`（缩写为 `np` 和 `pd`，这样我们就能写出更简短的代码）。库是一系列功能的集合，能让你在无需从头开始编写全部代码的情况下执行许多常见任务。\n",
    "\n",
    "例如，\"pandas \"库提供了 \"read_csv() \"函数，用于将逗号分隔值（csv）文件读入所谓的 \"数据帧\"。\n",
    "\n",
    "**提示**：您可以在本 Jupyter 笔记本中查看某个库的可用功能，方法是在库名后面写一个点，然后按 tab 键。所有可用功能都将弹出，供你探索。由于选项很多，你可以在弹出窗口时写上 \"阅读 \"等字样来缩小范围。\n",
    "\n",
    "**注意***：如果您使用的是 Google Colab，则必须首先在 \"工具\">\"设置\">\"编辑器 \"中禁用 \"自动触发代码补全\"，才能使用此功能。\n",
    "\n",
    "您可以通过 `pandas` 阅读所有可能的文件格式："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.read  # 光标位于 \"pd.read \"之后时，按下制表符键"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们执行（用 `Enter`）这个单元格，我们会得到一个 `AttributeError` 因为模块 `pandas` 不知道 `read()`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以使用 `?` 获取函数的 docstring，即关于该函数的作用和可以传递的参数类型的说明！"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们将使用 `read_csv()` 函数（[参见文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)）将 csv 文件内容作为 `DataFrame` 加载到变量 `data_raw` 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:3\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    715\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m _get_filepath_or_buffer(\n\u001b[1;32m    719\u001b[0m     path_or_buf,\n\u001b[1;32m    720\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m    721\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    722\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    723\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    724\u001b[0m )\n\u001b[1;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    727\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:372\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    371\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urlopen(req_info) \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    373\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:274\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# `read_csv`可以读取计算机中的路径，也可以读取互联网 URL！\n",
    "# 读取远程 csv 文件只需几秒钟\n",
    "data_raw = pd.read_csv(\"https://opendata.arcgis.com/datasets/dd4580c810204019a7b8eb3e0b329dd6_0.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们来看看 `data_raw` 中的 `DataFrame` 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.查看数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `DataFrame` 头/尾\n",
    "\n",
    "让我们使用 `head()` 函数（[参见文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html)）看看表格的前几行。\n",
    "\n",
    "**注意**：为了避免在本 Jupyter 笔记本中打印大型表格，我们会经常使用该命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.head()  # Shows by default the first 5 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们使用 `tail()` 函数（[参见文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html)）看看表格的最后几行。请注意，您可以向 `head()` 和 `tail()` 函数传递一个数字，以指定要查看的第一行/最后一行的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `DataFrame` 维度\n",
    "\n",
    "让我们使用 `shape` 以 `（行数，列数）` 的形式显示表格的列数和行数（= 维度/形状）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `DataFrame` 列名\n",
    "\n",
    "我们可以使用 `columns`获取所有列名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list here the meaning of a few criteria (see full list on [RKI COVID-19 data download website](https://www.arcgis.com/home/item.html?id=dd4580c810204019a7b8eb3e0b329dd6)):\n",
    "\n",
    "- `Bundesland`: State name\n",
    "- `Landkreis`: District name\n",
    "- `Altersgruppe`: Age group (6 groups: `0-4`, `5-14`, `15-34`, `35-59`, `60-79`, `80+` and `unbekannt`=unknown)\n",
    "- `Geschlecht`: Gender (`M`=male, `W`=female and `unbekannt`=unknown)\n",
    "- `AnzahlFall`: Number of cases in group\n",
    "- `AnzahlTodesfall`: Number of deaths in group\n",
    "- `AnzahlGenesen`: Number of recoveries cases in group\n",
    "- `Meldedatum`: Date when case was reported to the Gesundheitsamt (you will use this in the next lesson on data visualization with `matplotlib`)\n",
    "- `Datenstand`: Date when data was updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以把 \"DataFrame \"看作是一个列表（每个列表可以包含不同的数据类型），它以表格的形式显示，并带有列名和索引名等元数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lists = [['Helen', 20, 'female'], ['Paul', 25, 'male'], ['Kim', 35, 'female']]\n",
    "list_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list_of_lists, columns=['name', 'age', 'gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Your turn_：练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1__: 获取 (a) `data_raw` 中的前 4 行和 (b) 最后 5 行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2__: 获取 (a) `data_raw` 中的列数和 (b) 第三列名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3__: 建立一个包含 4 个国家数据的 `DataFrame` ：\n",
    "\n",
    "- 国家名称\n",
    "- 您最喜欢的国家\n",
    "- 您去过那里吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❤️接下来，我不想用中文了，我本来想锻炼自己的英语的，😭，你们就看英语吧❤️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Select columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By column name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select some interesting columns! The `DataFrame` is quite large and we are only interested in a subset of the offered criteria. With `pandas`, it is very easy to slice the columns that you want by the following syntax:\n",
    "\n",
    "```python\n",
    "data_raw[list_of_interesting_columns]\n",
    "```\n",
    "\n",
    "The list of column names of interest could look like this:\n",
    "```python\n",
    "list_of_interesting_columns = ['Bundesland', 'Landkreis']\n",
    "```\n",
    "\n",
    "Taking both steps together it looks like this (note the two sets of `[]`, the inner `[]` is part of the list, the outer `[]` is the syntax for `DataFrame` slicing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw[['Bundesland', 'Landkreis']].head()  # Note the use of .head() to show only the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see in the following that it is possible to write a command over multiple lines to make is easier to read.\n",
    "\n",
    "Let's write this operation's output into the variable `data`; we will use this variable from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw[\n",
    "    [\n",
    "        'Bundesland', \n",
    "        'Landkreis', \n",
    "        'Altersgruppe', \n",
    "        'Geschlecht', \n",
    "        'AnzahlFall', \n",
    "        'AnzahlTodesfall', \n",
    "        'AnzahlGenesen', \n",
    "        'Datenstand'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have noticed that there is no cell output as before. This happens when the output is saved in a variable (here `data`). Let's inspect the content of `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By column AND index names/indices using `loc/iloc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Recap__\n",
    "\n",
    "So far we sliced columns using column names like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Bundesland', 'Landkreis']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __`loc`__\n",
    "\n",
    "The above code is a shorter form for using `loc`:\n",
    "```python\n",
    "dataframe.loc[list_of_row_names, list_of_column_names]\n",
    "```\n",
    "`index_names` or `column_names` can be set to `:` if we want to select the full row or column, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, ['Bundesland', 'Landkreis']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. __`iloc`__\n",
    "\n",
    "Or, instead of row and column names, we can use their indices (like you learnt on day 1 where you selected elements from a list). \n",
    "\n",
    "```python\n",
    "dataframe.iloc[list_of_row_indices, list_of_column_indices].head()\n",
    "```\n",
    "\n",
    "Remember, in Python indices are 0-indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out index of columns of interest\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:, [2, 3]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: You will use `loc/iloc` in the notebooks to come in the next lessons, but for this lesson here, we will use column selection by column names as discussed first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Bundesland', 'Landkreis']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Your turn_: Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 4__: Select the columns listing the number of cases, deaths and recoveries using their __column names__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 5__: Do the same as in Exercise 4 but this time use __`loc`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 6__: Do the same as in Exercise 4 and 5 but this time use __`iloc`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Get unique entries in a column\n",
    "\n",
    "Now, we'd like to check what kind of entries we can find in a column. \n",
    "\n",
    "First, we select a column, similar to how we learned it in *Chapter 5.4*. Since we select this time only **one** column, we do not pass the column name as a list but as a simple string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Bundesland'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a `Series` (instead of a `DataFrame`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data['Bundesland'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the `unique()` function ([see docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unique.html)) and check the states in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Bundesland'].unique()  # Note: Here we pass the single column as string not as list (as shown in Chapter 5.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be 16 states, let's check with Python's built-in function `len` ([see docs](https://docs.python.org/3/library/functions.html#len)) that returns the length of e.g. list-like objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['Bundesland'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Your turn_: Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 7__: Select the column on age groups (`'Altersgruppe'`) - which age groups are monitored?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 8__: Select the column on districts (`'Landkreis'`) - how many districts are monitored?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6. Select rows (by conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very often, you not only have more criteria (columns) in your dataset than you are actually interested in but also more data points (rows) than you need. Let's say for instance, that we are mainly interested in data points regarding Berlin. Since we have a dataset for Germany, we will need to do some (row) filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select only the state column (`Bundesland`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Bundesland']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `Series` it is very easy to check for each row if it fullfils a given condition. As an example, let's ask for \"Thüringen\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Bundesland'] == 'Thüringen'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see, that this operation returns a `Series` of the same length and index as our initial `Series` containing boolean values (`True` or `False`).\n",
    "\n",
    "How can we use this boolean Series know to subset `data` for data points concerning Berlin (i.e. filter `data` for rows concerning Berlin)? We use the following syntax:\n",
    "\n",
    "```python\n",
    "data[condition]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition\n",
    "state_is_berlin = data['Bundesland'] == 'Berlin'\n",
    "\n",
    "# Subset dataset by condition\n",
    "data[state_is_berlin]  # equals\n",
    "data[data['Bundesland'] == 'Berlin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Your turn_: Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 9__: Select only data points for Berlin Mitte (one condition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 10__: Select only data points for Berlin and patients between 35 and 59 years old (two conditions).\n",
    "\n",
    "```python\n",
    "# Use one condition\n",
    "data[condition]\n",
    "\n",
    "# Use multiple conditions\n",
    "data[condition1 & condition2]  # Fullfill condition 1 AND 2\n",
    "data[condition1 & not condition2]  # Fullfill condition 1 AND not 2\n",
    "data[condition1 | condition2]  # Fullfill condition 1 OR 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7. Group data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on, we will continue to work only with data for Berlin, so we will save the subset to the new variable `data_berlin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_berlin = data[data['Bundesland'] == 'Berlin']\n",
    "data_berlin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html:\n",
    "\n",
    "> By `groupby()` we are referring to a process involving one or more of the following steps:\n",
    "> * **Splitting** the data into groups based on some criteria.\n",
    "> * **Applying** a function to each group independently.\n",
    "> * **Combining** the results into a data structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Get group sum with `sum()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting**: Split data into groups based on a criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_berlin.groupby('Altersgruppe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_berlin.groupby('Altersgruppe'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at one of the groups (= subset of the full DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_berlin.groupby('Altersgruppe').get_group(\"A00-A04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying and combining**: Apply function to each group, e.g. get the sum of numerical values in each group using `sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_berlin.groupby('Altersgruppe').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `pandas` it is very easy to quickly plot data using the `plot()` function ([see docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html)) - with the parameter `kind` you can specify what plot type you want to plot (in our case we want a barplot). Note that the index labels will serve as x-axis labels.\n",
    "\n",
    "Select `AnzahlFall` for the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_berlin.groupby('Altersgruppe').sum()['AnzahlFall'].plot(\n",
    "    kind='bar', \n",
    "    title=f'Number of all COVID19 cases in Berlin since the beginning of the pandemic ({data[\"Datenstand\"].unique()[0]})'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this plot with the [RKI Dashboard](https://corona.rki.de/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Your turn_: Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 11__: Since the `groupby` functionality is very powerful but also at first difficult to wraps our head around, go through the first two examples above again in your group and discuss questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 12__: Get number of total COVID-19 cases by Berlin's districts and compare your findings to the [official COVID-19 table for Berlin > Bezirke > Übersicht](https://www.berlin.de/corona/lagebericht/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 13__: Plot the number of total COVID-19 cases in Berlin grouped by Berlin's districts (barplot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discussion\n",
    "\n",
    "In this notebook, we saw how quickly possible it is to read in a csv file as `DataFrame` (*Chapter 5.2*) and to start working with it. \n",
    "- We got a first impression on our COVID-19 Berlin dataset. We looked at the number of data points (`DataFrame` rows) and criteria (`DataFrame` columns) as well as some example data points, see *Chapter 5.3*.\n",
    "- We selected interesting columns and checked what kind of column entries we can except, see *Chapter 5.4 and 5.5*. \n",
    "- We grouped data by certain criteria (columns), and applied operations on these groups, e.g. we calculated the sum within each group). We also did some first steps towards plotting with `pandas`, see *Chapter 5.6*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final exercise\n",
    "\n",
    "As promised at the beginning, you will get your own dataset now :)\n",
    "\n",
    "Last year during the course, we could only work with COVID-19 cases data but luckily, this year, we have something positive to look at as well - the vaccination progress in Germany! You can find that data online again at the [RKI website](https://www.rki.de/DE/Content/InfAZ/N/Neuartiges_Coronavirus/Daten/Impfquoten-Tab.html) (under the \"Daten\" section). \n",
    "\n",
    "The provided Excel file is a bit difficult to handle, thus many GitHub repos have been set up to process the dataset into formats that are easier to work with, e.g. https://github.com/ard-data/2020-rki-impf-archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let's load the cumulative vaccination progress for Germany. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccination_cumulative = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/ard-data/2020-rki-impf-archive/master/data/9_csv_v3/region_DE.csv\"\n",
    ")\n",
    "vaccination_cumulative.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Let the `date` column know that it represents dates (change data structure from `object` to `datetime`). This will help us later during plotting because `pandas` will not try to label each day in the plot but maybe rather every month (depending on the range of dates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccination_cumulative[\"date\"] = pd.to_datetime(vaccination_cumulative[\"date\"])\n",
    "vaccination_cumulative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccination_cumulative.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Set the date as the `DataFrame` index. Use `your_dataframe.set_index(column_name)` for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Select only the columns containing the cumulative number of people who are fully vaccinated or vaccinated once/twice (`personen_voll_kumulativ`, `personen_erst_kumulativ`, and `personen_zweit_kumulativ`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Plot the cumulative time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Compare your results to the data on the BMG website: https://impfdashboard.de/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solutions__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Words of encouragement :)__ \n",
    "\n",
    "Before you take a look at the solutions, try to solve the exercises yourself. \n",
    "\n",
    "All the information needed lives in _5. Practical_ - if you are stuck, first take a look at the material there. Talk to your fellow students. If you have a solution, then go ahead and take a look here.\n",
    "\n",
    "Also note that the solutions given here show only one possibility - most of the times there are multiple options to achieve the same end result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> > Solution 1</summary>\n",
    "    \n",
    "```python\n",
    "data.head(4)\n",
    "data.tail()\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> > Solution 2</summary>\n",
    "    \n",
    "```python\n",
    "len(data.columns)\n",
    "data.columns[2]\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> > Solution 3</summary>\n",
    "    \n",
    "```python\n",
    "pd.DataFrame(\n",
    "    [\n",
    "        [\"France\", \"Gewürztraminer\", True], \n",
    "        [\"Australia\", \"beautiful nature\", True], \n",
    "        [\"Israel\", \"hummus\", True], \n",
    "        [\"Iceland\", \"language\", False]\n",
    "    ], \n",
    "    columns=[\"country\", \"awesome because of\", \"been there\"]\n",
    ")\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> > Solution 4</summary>\n",
    "    \n",
    "```python\n",
    "data[[\"AnzahlFall\", \"AnzahlTodesfall\", \"AnzahlGenesen\"]]\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> > Solution 5</summary>\n",
    "    \n",
    "```python\n",
    "data.loc[:, [\"AnzahlFall\", \"AnzahlTodesfall\", \"AnzahlGenesen\"]]\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> > Solution 6</summary>\n",
    "    \n",
    "```python\n",
    "data.iloc[:, [4, 5, 6]]\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> > Solution 7</summary>\n",
    "    \n",
    "```python\n",
    "data[\"Altersgruppe\"].unique()\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> > Solution 8</summary>\n",
    "    \n",
    "```python\n",
    "len(data[\"Landkreis\"].unique())\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> > Solution 9</summary>\n",
    "    \n",
    "```python\n",
    "data[data[\"Landkreis\"] == \"SK Berlin Mitte\"]\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> > Solution 10</summary>\n",
    "    \n",
    "```python\n",
    "data[\n",
    "    (data[\"Bundesland\"] == \"Berlin\") & \n",
    "    (data[\"Altersgruppe\"] == \"A35-A59\")\n",
    "]\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> > Solution 11</summary>\n",
    "    \n",
    "Go through _Chapter 5.6._ one more time.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> > Solution 12</summary>\n",
    "    \n",
    "```python\n",
    "data_berlin.groupby('Landkreis')['AnzahlFall'].sum()\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> > Solution 13</summary>\n",
    "    \n",
    "```python\n",
    "data_berlin.groupby('Landkreis')['AnzahlFall'].sum().plot(\n",
    "    kind='bar', title=f'Number of COVID-19 cases in Berlin'\n",
    ")\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> > Solution to final exercise</summary>\n",
    "    \n",
    "```python\n",
    "vaccination_cumulative = vaccination_cumulative.set_index(\"date\")\n",
    "vaccination_cumulative = vaccination_cumulative[[\"personen_erst_kumulativ\", \"personen_voll_kumulativ\", \"personen_zweit_kumulativ\"]]\n",
    "vaccination_cumulative.plot()\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
